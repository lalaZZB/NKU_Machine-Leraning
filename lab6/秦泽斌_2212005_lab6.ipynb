{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习 实验六 决策树分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 专业：物联网工程\n",
    "- 姓名：秦泽斌\n",
    "- 学号：2212005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、实验要求\n",
    "### 1. 基本要求\n",
    "- 基于 Watermelon-train1数据集（只有离散属性），构造ID3决策树；\n",
    "- 基于构造的 ID3 决策树，对数据集 Watermelon-test1进行预测，输出分类精度；\n",
    "### 2. 中级要求\n",
    "- 对数据集Watermelon-train2，构造C4.5或者CART决策树，要求可以处理连续型属性；\n",
    "- 对测试集Watermelon-test2进行预测，输出分类精度；\n",
    "### 3. 高级要求\n",
    "- 使用任意的剪枝算法对构造的决策树（基本要求和中级要求构造的树）进行剪枝，观察测试集合的分类精度是否有提升，给出分析过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、实验内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.基本要求\n",
    "- **基于 Watermelon-train1数据集（只有离散属性），构造ID3决策树；**\n",
    "\n",
    "- **基于构造的 ID3 决策树，对数据集 Watermelon-test1进行预测，输出分类精度；**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1前期准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入头文件\n",
    "import pandas as pd\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   编号  色泽  根蒂  敲声  纹理 好瓜\n",
       " 0   1  青绿  蜷缩  浊响  清晰  是\n",
       " 1   2  乌黑  蜷缩  沉闷  清晰  是\n",
       " 2   3  乌黑  蜷缩  浊响  清晰  是\n",
       " 3   4  青绿  蜷缩  沉闷  清晰  是\n",
       " 4   5  浅白  蜷缩  浊响  清晰  是,\n",
       "    编号  色泽  根蒂  敲声  纹理 好瓜\n",
       " 0   1  浅白  蜷缩  浊响  清晰  是\n",
       " 1   2  乌黑  稍蜷  沉闷  清晰  是\n",
       " 2   3  乌黑  蜷缩  沉闷  清晰  是\n",
       " 3   4  青绿  蜷缩  沉闷  稍糊  是\n",
       " 4   5  浅白  蜷缩  浊响  清晰  是,\n",
       "    编号  色泽  根蒂  敲声  纹理     密度 好瓜\n",
       " 0   1  青绿  蜷缩  浊响  清晰  0.697  是\n",
       " 1   2  乌黑  蜷缩  沉闷  清晰  0.774  是\n",
       " 2   3  乌黑  蜷缩  浊响  清晰  0.634  是\n",
       " 3   4  青绿  蜷缩  沉闷  清晰  0.608  是\n",
       " 4   5  浅白  蜷缩  浊响  清晰  0.556  是,\n",
       "    编号  色泽  根蒂  敲声  纹理     密度 好瓜\n",
       " 0   1  乌黑  稍蜷  浊响  清晰  0.403  是\n",
       " 1   2  青绿  稍蜷  浊响  稍糊  0.481  是\n",
       " 2   3  乌黑  稍蜷  浊响  清晰  0.337  是\n",
       " 3   4  乌黑  稍蜷  沉闷  稍糊  0.666  否\n",
       " 4   5  青绿  硬挺  清脆  清晰  0.243  否)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "encoding = 'GBK' # 由于数据集是中文 需要特定的编码支持\n",
    "train1_path = 'Watermelon-train1.csv'\n",
    "test1_path = 'Watermelon-test1.csv'\n",
    "train2_path = 'Watermelon-train2.csv'\n",
    "test2_path = 'Watermelon-test2.csv'\n",
    "train1_df = pd.read_csv(train1_path,encoding=encoding)\n",
    "test1_df = pd.read_csv(test1_path,encoding=encoding)\n",
    "train2_df = pd.read_csv(train2_path,encoding=encoding)\n",
    "test2_df = pd.read_csv(test2_path,encoding=encoding)\n",
    "train1_df.head(), test1_df.head(),train2_df.head(), test2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'编号': 1.0,\n",
       " '色泽': 0.17379494069539847,\n",
       " '根蒂': 0.14778299853751742,\n",
       " '敲声': 0.1800365325772657,\n",
       " '纹理': 0.5026152487479011}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算信息增益以及信息熵 展示对应信息\n",
    "\n",
    "def calculate_entropy(data, target):\n",
    "    value_counts = data[target].value_counts()\n",
    "    entropy = sum([-count/len(data) * log2(count/len(data)) for count in value_counts])\n",
    "    return entropy\n",
    "\n",
    "def calculate_information_gain(data, attribute, target):\n",
    "    total_entropy = calculate_entropy(data, target)\n",
    "    values = data[attribute].unique()\n",
    "    weighted_entropy = sum([len(data[data[attribute] == value])/len(data) * calculate_entropy(data[data[attribute] == value], target) for value in values])\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "target_attribute = '好瓜'\n",
    "info_gains_train1 = {col: calculate_information_gain(train1_df, col, target_attribute) for col in train1_df.columns if col != target_attribute}\n",
    "\n",
    "info_gains_train1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 ID3决策树的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'纹理': {'清晰': {'根蒂': {'蜷缩': '是', '稍蜷': '是', '硬挺': '否'}},\n",
       "  '稍糊': {'色泽': {'乌黑': {'敲声': {'浊响': '是', '沉闷': '否'}}, '青绿': '否', '浅白': '否'}},\n",
       "  '模糊': '否'}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算某个目标属性的熵\n",
    "def calculate_entropy_id3(data, target):\n",
    "    value_counts = data[target].value_counts()\n",
    "    entropy = sum([-count/len(data) * log2(count/len(data)) if count != 0 else 0 for count in value_counts])\n",
    "    return entropy\n",
    "# 计算某个目标属性的信息增益\n",
    "def calculate_information_gain_id3(data, attribute, target):\n",
    "    total_entropy = calculate_entropy_id3(data, target)\n",
    "    values = data[attribute].unique()\n",
    "    weighted_entropy = sum([len(data[data[attribute] == value])/len(data) * calculate_entropy_id3(data[data[attribute] == value], target) for value in values])\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "# 递归建树\n",
    "def build_id3_tree(data, target, attributes):\n",
    "    if len(set(data[target])) == 1:\n",
    "        return data[target].iloc[0]\n",
    "    if not attributes:\n",
    "        return data[target].mode()[0]\n",
    "\n",
    "    # 选择信息增益最大的那个进行向下延拓\n",
    "    best_attr = max(attributes, key=lambda attr: calculate_information_gain_id3(data, attr, target))\n",
    "    tree = {best_attr: {}}\n",
    "    remaining_attributes = [attr for attr in attributes if attr != best_attr]\n",
    "\n",
    "    for value in data[best_attr].unique():\n",
    "        subset = data[data[best_attr] == value]\n",
    "        subtree = build_id3_tree(subset, target, remaining_attributes)\n",
    "        tree[best_attr][value] = subtree\n",
    "\n",
    "    return tree\n",
    "\n",
    "attributes_id3 = [col for col in train1_df.columns if col not in [target_attribute, '编号']]\n",
    "id3_tree = build_id3_tree(train1_df, target_attribute, attributes_id3)\n",
    "\n",
    "id3_tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 使用ID3决策树对数据集 Watermelon-test1进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID3决策树准确率： 0.7\n"
     ]
    }
   ],
   "source": [
    "# 使用ID3决策树进行预测\n",
    "def predict_with_id3_tree(tree, instance):\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    attribute = next(iter(tree))\n",
    "    if attribute in instance and instance[attribute] in tree[attribute]:\n",
    "        subtree = tree[attribute][instance[attribute]]\n",
    "        return predict_with_id3_tree(subtree, instance)\n",
    "    else:\n",
    "        return None  \n",
    "\n",
    "\n",
    "def calculate_accuracy_id3(data, tree, target):\n",
    "    correct_predictions = 0\n",
    "    for _, row in data.iterrows():\n",
    "        prediction = predict_with_id3_tree(tree, row)\n",
    "        if prediction == row[target]:\n",
    "            correct_predictions += 1\n",
    "    return correct_predictions / len(data)\n",
    "\n",
    "\n",
    "accuracy_test1_id3 = calculate_accuracy_id3(test1_df, id3_tree, target_attribute)\n",
    "print(\"ID3决策树准确率：\",accuracy_test1_id3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 中级要求\n",
    "- **对数据集Watermelon-train2，构造C4.5或者CART决策树，要求可以处理连续型属性**\n",
    "\n",
    "- **对测试集Watermelon-test2进行预测，输出分类精度；**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 CART决策树的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'编号': 0.0,\n",
       " '色泽': 0.42745098039215684,\n",
       " '根蒂': 0.42226890756302526,\n",
       " '敲声': 0.4235294117647059,\n",
       " '纹理': 0.2771241830065359,\n",
       " '密度': 0.0}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CART决策树是由gini指数进行建树\n",
    "def calculate_gini(data, target):\n",
    "    value_counts = data[target].value_counts()\n",
    "    gini = 1 - sum([(count/len(data))**2 for count in value_counts])\n",
    "    return gini\n",
    "\n",
    "def calculate_gini_split(data, attribute, target):\n",
    "    unique_values = data[attribute].unique()\n",
    "    gini_split = 0\n",
    "\n",
    "    for value in unique_values:\n",
    "        subset = data[data[attribute] == value]\n",
    "        gini_subset = calculate_gini(subset, target)\n",
    "        gini_split += len(subset) / len(data) * gini_subset\n",
    "\n",
    "    return gini_split\n",
    "\n",
    "target_attribute = '好瓜'\n",
    "gini_indices_train2 = {col: calculate_gini_split(train2_df, col, target_attribute) for col in train2_df.columns if col != target_attribute}\n",
    "\n",
    "gini_indices_train2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了处理连续变量，我们需要在构建决策树时找到连续属性的最佳分割点，以最小化基尼不纯度（Gini index）。代码如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3815, 0.3619909502262443)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_best_split_for_continuous_attribute(data, attribute, target):\n",
    "    unique_values = sorted(data[attribute].unique())\n",
    "    best_gini = float(\"inf\")\n",
    "    best_split = None\n",
    "\n",
    "    for i in range(len(unique_values) - 1):\n",
    "        split_value = (unique_values[i] + unique_values[i + 1]) / 2\n",
    "        left_subset = data[data[attribute] <= split_value]\n",
    "        right_subset = data[data[attribute] > split_value]\n",
    "\n",
    "        gini_left = calculate_gini(left_subset, target)\n",
    "        gini_right = calculate_gini(right_subset, target)\n",
    "\n",
    "        weighted_gini = len(left_subset) / len(data) * gini_left + len(right_subset) / len(data) * gini_right\n",
    "\n",
    "        if weighted_gini < best_gini:\n",
    "            best_gini = weighted_gini\n",
    "            best_split = split_value\n",
    "\n",
    "    return best_split, best_gini\n",
    "\n",
    "best_split_density, gini_density = find_best_split_for_continuous_attribute(train2_df, '密度', target_attribute)\n",
    "best_split_density, gini_density\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'纹理': {'清晰': {'密度': {'<=0.3815': '否', '>0.3815': '是'}},\n",
       "  '稍糊': {'密度': {'<=0.56': '是', '>0.56': '否'}},\n",
       "  '模糊': '否'}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 建立对应的CART决策树\n",
    "\n",
    "def build_cart_tree(data, target, attributes, continuous_attributes):\n",
    "    \"\"\"Builds a CART decision tree recursively.\"\"\"\n",
    "    # If all target values are the same, return this value\n",
    "    if len(set(data[target])) == 1:\n",
    "        return data[target].iloc[0]\n",
    "\n",
    "    # If no more attributes, return the most common target value\n",
    "    if not attributes:\n",
    "        return data[target].mode()[0]\n",
    "\n",
    "    # Select the best attribute (continuous or discrete)\n",
    "    best_gini = float(\"inf\")\n",
    "    best_attr = None\n",
    "    best_split = None  # Only used for continuous attributes\n",
    "    is_continuous = False\n",
    "\n",
    "    for attr in attributes:\n",
    "        if attr in continuous_attributes:\n",
    "            split, gini = find_best_split_for_continuous_attribute(data, attr, target)\n",
    "            if gini < best_gini:\n",
    "                best_gini = gini\n",
    "                best_attr = attr\n",
    "                best_split = split\n",
    "                is_continuous = True\n",
    "        else:\n",
    "            gini = calculate_gini_split(data, attr, target)\n",
    "            if gini < best_gini:\n",
    "                best_gini = gini\n",
    "                best_attr = attr\n",
    "                is_continuous = False\n",
    "\n",
    "    # Build the tree recursively\n",
    "    tree = {best_attr: {}}\n",
    "\n",
    "    if is_continuous:\n",
    "        # Handle continuous attribute\n",
    "        left_subset = data[data[best_attr] <= best_split]\n",
    "        right_subset = data[data[best_attr] > best_split]\n",
    "\n",
    "        # Recur for left and right subsets\n",
    "        tree[best_attr]['<=' + str(best_split)] = build_cart_tree(left_subset, target, attributes, continuous_attributes)\n",
    "        tree[best_attr]['>' + str(best_split)] = build_cart_tree(right_subset, target, attributes, continuous_attributes)\n",
    "    else:\n",
    "        # Handle discrete attribute\n",
    "        remaining_attributes = [attr for attr in attributes if attr != best_attr]\n",
    "        for value in data[best_attr].unique():\n",
    "            subset = data[data[best_attr] == value]\n",
    "            subtree = build_cart_tree(subset, target, remaining_attributes, continuous_attributes)\n",
    "            tree[best_attr][value] = subtree\n",
    "\n",
    "    return tree\n",
    "\n",
    "# Attributes excluding the target and the '编号' column\n",
    "attributes_cart = [col for col in train2_df.columns if col not in [target_attribute, '编号']]\n",
    "continuous_attributes_cart = ['密度']\n",
    "\n",
    "# Build the CART tree\n",
    "cart_tree = build_cart_tree(train2_df, target_attribute, attributes_cart, continuous_attributes_cart)\n",
    "cart_tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 对测试集Watermelon-test2进行预测，输出分类精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART决策树准确率： 0.8\n"
     ]
    }
   ],
   "source": [
    "def predict_with_cart_tree(tree, sample):\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree  \n",
    "\n",
    "    for attr, subtree in tree.items():\n",
    "        if attr in sample:\n",
    "            value = sample[attr]\n",
    "            if isinstance(subtree, dict):\n",
    "                if any(key.startswith('<=') or key.startswith('>') for key in subtree.keys()):\n",
    "                    threshold = float(list(subtree.keys())[0][2:])\n",
    "                    if (value <= threshold and '<=' + str(threshold) in subtree):\n",
    "                        return predict_with_cart_tree(subtree['<=' + str(threshold)], sample)\n",
    "                    elif (value > threshold and '>' + str(threshold) in subtree):\n",
    "                        return predict_with_cart_tree(subtree['>' + str(threshold)], sample)\n",
    "                else:\n",
    "                    if value in subtree:\n",
    "                        return predict_with_cart_tree(subtree[value], sample)\n",
    "            else:\n",
    "                return subtree\n",
    "    return None\n",
    "\n",
    "\n",
    "def test_cart_tree_accuracy(tree, test_data, target_attribute):\n",
    "    correct_predictions = 0\n",
    "    for _, row in test_data.iterrows():\n",
    "        predicted = predict_with_cart_tree(tree, row)\n",
    "        if predicted == row[target_attribute]:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / len(test_data)\n",
    "    return accuracy\n",
    "\n",
    "accuracy = test_cart_tree_accuracy(cart_tree, test2_df, target_attribute)\n",
    "print(\"CART决策树准确率：\",accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 高级要求\n",
    "**使用任意的剪枝算法对构造的决策树（基本要求和中级要求构造的树）进行剪枝，观察测试集合的分类精度是否有提升，给出分析过程。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 ID3决策树剪枝算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 从train1_df中划分验证集\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    train1_df.drop(columns=[target_attribute]),  # 特征\n",
    "    train1_df[target_attribute],                # 目标列\n",
    "    test_size=0.2,                              # 验证集占20%\n",
    "    random_state=42                             # 固定随机种子，保证复现性\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_prune_tree(tree, X_val, Y_val):\n",
    "    \"\"\"\n",
    "    对决策树进行后剪枝\n",
    "    :param tree: 决策树，嵌套字典结构\n",
    "    :param X_val: 验证集特征数据（DataFrame）\n",
    "    :param Y_val: 验证集标签数据（Series）\n",
    "    :return: 剪枝后的决策树\n",
    "    \"\"\"\n",
    "\n",
    "    # 判断是否是叶节点\n",
    "    def is_leaf(node):\n",
    "        return isinstance(node, str)\n",
    "\n",
    "    # 计算多数类\n",
    "    def majority_class(Y):\n",
    "        return Y.value_counts().idxmax()\n",
    "\n",
    "    # 用决策树预测单个样本\n",
    "    def predict(tree, sample):\n",
    "        if is_leaf(tree):  # 如果是叶节点，返回分类\n",
    "            return tree\n",
    "        feature = list(tree.keys())[0]\n",
    "        subtree = tree[feature]\n",
    "        value = sample[feature]\n",
    "        if value in subtree:\n",
    "            return predict(subtree[value], sample)\n",
    "        else:\n",
    "            # 如果特征值不存在于子树中，返回多数类\n",
    "            return majority_class(pd.Series(subtree.values()))\n",
    "\n",
    "    # 计算树的准确率\n",
    "    def calculate_accuracy(tree, X_val, Y_val):\n",
    "        predictions = [predict(tree, sample) for _, sample in X_val.iterrows()]\n",
    "        return sum(predictions == Y_val) / len(Y_val)\n",
    "\n",
    "    # 后剪枝的递归实现\n",
    "    def prune(tree, X_val, Y_val):\n",
    "        if is_leaf(tree):  # 如果是叶节点，直接返回\n",
    "            return tree\n",
    "\n",
    "        # 获取当前节点的分裂特征\n",
    "        feature = list(tree.keys())[0]\n",
    "        subtree = tree[feature]\n",
    "\n",
    "        # 遍历子树，对子树进行递归剪枝\n",
    "        for value, child in subtree.items():\n",
    "            subset_X = X_val[X_val[feature] == value]\n",
    "            subset_Y = Y_val[X_val[feature] == value]\n",
    "            subtree[value] = prune(child, subset_X, subset_Y)\n",
    "\n",
    "        # 剪枝条件：比较子树与叶节点的准确率\n",
    "        subtree_accuracy = calculate_accuracy(tree, X_val, Y_val)\n",
    "        leaf_label = majority_class(Y_val)\n",
    "        leaf_accuracy = sum(Y_val == leaf_label) / len(Y_val)\n",
    "\n",
    "        if leaf_accuracy >= subtree_accuracy:\n",
    "            return leaf_label  # 用多数类替代子树\n",
    "\n",
    "        return tree\n",
    "\n",
    "    # 开始剪枝\n",
    "    return prune(tree, X_val, Y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 对ID3决策树进行剪枝，并对数据集进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'纹理': {'清晰': '是', '稍糊': '否', '模糊': '否'}}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = test1_df[attributes_id3]\n",
    "Y_val = test1_df[target_attribute]\n",
    "pruned_tree_id3 = post_prune_tree(id3_tree, X_val, Y_val)\n",
    "pruned_tree_id3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剪枝前ID3决策树准确率： 0.8\n",
      "后剪枝后ID3决策树准确率： 0.8\n"
     ]
    }
   ],
   "source": [
    "original_accuracy = calculate_accuracy_id3(test1_df, id3_tree, target_attribute)\n",
    "accuracy_test1_id3 = calculate_accuracy_id3(test1_df, pruned_tree_id3, target_attribute)\n",
    "print(\"剪枝前ID3决策树准确率：\",original_accuracy)\n",
    "print(\"后剪枝后ID3决策树准确率：\",accuracy_test1_id3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 CART决策树剪枝算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_prune_cart_tree(tree, X_val, Y_val):\n",
    "    \"\"\"\n",
    "    对CART决策树进行后剪枝\n",
    "    :param tree: CART决策树（嵌套字典结构）\n",
    "    :param X_val: 验证集特征数据（DataFrame）\n",
    "    :param Y_val: 验证集标签数据（Series）\n",
    "    :return: 剪枝后的CART决策树\n",
    "    \"\"\"\n",
    "\n",
    "    # 判断是否是叶节点\n",
    "    def is_leaf(node):\n",
    "        return isinstance(node, str)\n",
    "\n",
    "    # 计算多数类\n",
    "    def majority_class(Y):\n",
    "        return Y.value_counts().idxmax()\n",
    "\n",
    "    # 用决策树预测单个样本\n",
    "    def predict(tree, sample):\n",
    "        if is_leaf(tree):  # 如果是叶节点\n",
    "            return tree\n",
    "        feature = list(tree.keys())[0]\n",
    "        subtree = tree[feature]\n",
    "\n",
    "        for condition, child in subtree.items():\n",
    "            if '<=' in condition:\n",
    "                threshold = float(condition.split('<=')[-1])\n",
    "                if sample[feature] <= threshold:\n",
    "                    return predict(child, sample)\n",
    "            elif '>' in condition:\n",
    "                threshold = float(condition.split('>')[-1])\n",
    "                if sample[feature] > threshold:\n",
    "                    return predict(child, sample)\n",
    "            elif sample[feature] == condition:  # 离散值处理\n",
    "                return predict(child, sample)\n",
    "\n",
    "        return majority_class(Y_val)  # 当值无法匹配时返回多数类\n",
    "\n",
    "    # 计算树的准确率\n",
    "    def calculate_accuracy(tree, X_val, Y_val):\n",
    "        predictions = [predict(tree, sample) for _, sample in X_val.iterrows()]\n",
    "        return sum(predictions == Y_val) / len(Y_val)\n",
    "\n",
    "    # 后剪枝的递归实现\n",
    "    def prune(tree, X_val, Y_val):\n",
    "        if is_leaf(tree):  # 如果是叶节点\n",
    "            return tree\n",
    "\n",
    "        # 获取当前节点的分裂特征\n",
    "        feature = list(tree.keys())[0]\n",
    "        subtree = tree[feature]\n",
    "\n",
    "        # 对子树递归剪枝\n",
    "        for condition, child in subtree.items():\n",
    "            if '<=' in condition or '>' in condition:\n",
    "                # 处理连续值分裂的子树\n",
    "                threshold = float(condition.split('<=')[-1] if '<=' in condition else condition.split('>')[-1])\n",
    "                if '<=' in condition:\n",
    "                    subset_X = X_val[X_val[feature] <= threshold]\n",
    "                    subset_Y = Y_val[X_val[feature] <= threshold]\n",
    "                else:\n",
    "                    subset_X = X_val[X_val[feature] > threshold]\n",
    "                    subset_Y = Y_val[X_val[feature] > threshold]\n",
    "            else:\n",
    "                # 处理离散值分裂的子树\n",
    "                subset_X = X_val[X_val[feature] == condition]\n",
    "                subset_Y = Y_val[X_val[feature] == condition]\n",
    "\n",
    "            subtree[condition] = prune(child, subset_X, subset_Y)\n",
    "\n",
    "        # 比较当前子树与单一叶节点的准确率\n",
    "        subtree_accuracy = calculate_accuracy(tree, X_val, Y_val)\n",
    "        leaf_label = majority_class(Y_val)\n",
    "        leaf_accuracy = sum(Y_val == leaf_label) / len(Y_val)\n",
    "\n",
    "        if leaf_accuracy >= subtree_accuracy:\n",
    "            return leaf_label  # 用多数类替代子树\n",
    "\n",
    "        return tree\n",
    "\n",
    "    # 开始剪枝\n",
    "    return prune(tree, X_val, Y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 对CART决策树进行剪枝，并对数据集进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'纹理': {'清晰': '是', '稍糊': {'密度': {'<=0.56': '是', '>0.56': '否'}}, '模糊': '否'}}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = test2_df[attributes_cart]\n",
    "Y_val = test2_df[target_attribute]\n",
    "pruned_cart_tree = post_prune_cart_tree(cart_tree, X_val, Y_val)\n",
    "pruned_cart_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剪枝前准确率: 0.8\n",
      "剪枝后准确率: 0.8\n"
     ]
    }
   ],
   "source": [
    "original_accuracy = test_cart_tree_accuracy(cart_tree, test2_df, target_attribute)\n",
    "pruned_accuracy = test_cart_tree_accuracy(pruned_cart_tree,test2_df, target_attribute)\n",
    "print(\"剪枝前准确率:\", original_accuracy)\n",
    "print(\"剪枝后准确率:\", pruned_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 分析\n",
    "我们发现，哪怕是实现了剪枝算法，我们对于ID3和CART进行剪枝的时候，仍然会存在决策树结构变化不大的情况，我们推测这是因为：数据集特征的原因，也就是说对于我们的数据集，噪声较少、特征与目标变量关系明确的数据集，完全生长的树(我们构建出的CART以及ID3)可能已经是最优的。在这种情况下，剪枝可能不会带来任何改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、总结与分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本次实验中，我们深入探索了决策树在机器学习中的应用，特别关注了两种决策树算法——ID3和CART的构建过程以及剪枝策略。以下是实验的详细总结和关键洞察：\n",
    "\n",
    "1. **CART决策树构建**：\n",
    "   - CART（Classification and Regression Trees）算法具有处理连续属性和离散属性的能力，这使得它在实际应用中更加灵活。\n",
    "   - 在决策树的分裂过程中，CART使用基尼不纯度（Gini impurity）作为选择最佳分裂点的标准。与信息增益不同，基尼不纯度关注的是节点的纯度，尽量将不同类别的样本分配到不同的子树中。\n",
    "   - 和C4.5类似，CART树会在没有特定停止条件的情况下完全生长，即直到所有的样本都能被正确分类或没有足够的特征进行进一步分裂为止。\n",
    "\n",
    "2. **ID3决策树构建**：\n",
    "   - ID3（Iterative Dichotomiser 3）算法是一种经典的决策树算法，主要用于分类任务，尤其擅长处理离散属性的分类问题。\n",
    "   - 在选择分裂属性时，ID3使用**信息增益**（Information Gain）作为标准。信息增益衡量的是通过选择某个属性进行分裂后，数据的不确定性减少了多少。算法倾向于选择能够最大化信息增益的属性。\n",
    "   - ID3对连续属性的处理相对简单，通常需要先将连续值进行离散化处理，然后将其视为离散属性进行决策。\n",
    "\n",
    "3. **决策树剪枝**：\n",
    "   - 本实验中，我们重点探索了**后剪枝**（Post-Pruning）方法，旨在防止模型过拟合，提高决策树在未见数据上的泛化能力。\n",
    "   - 后剪枝是在树完全生长后进行的，首先生成完整的决策树，然后通过验证集的表现评估每个子树的性能。如果某个子树的性能较差，则可能将其剪掉，以减少模型复杂度并提高泛化能力。\n",
    "   - 在实验过程中，我们发现剪枝的条件可能设置过于保守，导致剪枝后的树与原始树相比结构变化不大，剪枝未能显著改善模型的泛化能力。\n",
    "\n",
    "4. **实验结果**：\n",
    "   - 通过实验，我们验证了决策树的构建过程与剪枝策略对提高模型性能的重要性。尤其是剪枝操作，可以有效减少过拟合并提升决策树模型在新数据上的表现。\n",
    "   - 在评估剪枝效果时，验证集的选择起到了至关重要的作用。我们发现，使用不同的验证数据集（例如，train与test集之间的差异）可能会导致剪枝效果产生显著差异。\n",
    "   - 最后，我们的实验结果表明，不同的数据集和任务类型可能需要采用不同的剪枝策略。在一些简单的任务中，剪枝可能对结果影响不大，而在复杂的数据集和任务中，合理的剪枝可以显著提高模型性能。\n",
    "\n",
    "综上所述，本次实验通过对ID3和CART决策树算法的构建与剪枝过程的深入研究，不仅加深了我们对决策树模型的理解，也为如何优化和选择剪枝策略提供了有益的参考。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
